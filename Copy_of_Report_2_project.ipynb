{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Report_2_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6wgPrsDfI28bfnY1aKARB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavgairola/Anomaly_Detection_OTC_Market/blob/main/Copy_of_Report_2_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJEcEhDNzHbP"
      },
      "source": [
        "<h1>\n",
        "<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>\n",
        "\n",
        "<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n",
        "<script type=\"text/javascript\" id=\"MathJax-script\" async\n",
        "  src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js\">\n",
        "</script>\n",
        "<center><b> What twitter educates us about the stock pumps?\n",
        "</b></center>\n",
        "<center><b> A. Gairola\n",
        "</b></center>\n",
        "<center><b>\n",
        "</b></center>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1ZYaPNT-9939UEx0bk3ndlMMq7FMPsMIk \"alt=\"Loan Cartoon\" width=\"400\" height=\"300\">\n",
        "</center>\n",
        "</h1>\n",
        "\n",
        "\n",
        "<section>\n",
        "<h2>\n",
        "<b>Problem Definition</b>\n",
        "</h2>\n",
        "<a name=\"ref_sec_1\">\n",
        "<p>\n",
        "<a href= \"https://qspace.library.queensu.ca/bitstream/handle/1974/28239/David_Nam_K_202010_MSC.pdf?sequence=2&isAllowed=y\">Over the past several decades, advances in technology have significantly impacted all aspects of the financial system. While it has led to numerous benefits, it has also increased the methods for manipulating the market. A frequent platform used to perform these market manipulation schemes has been through social media. </a> Twitter is one such platform where people and potential \"pumpers\" also tweet about certain stocks to increase their price above a certain value. This is manipulative behavior and is known as a \"pump and dump\" scheme. The objective of this project to classify the stock pumps and which factors contribute to them. The null hypothesis I am making is that the tweet sentiments 'may' contribute towards it significantly.\n",
        "</p>\n",
        "</a>\n",
        "</section>\n",
        "\n",
        "\n",
        "\n",
        "<section>\n",
        "<h2>\n",
        "<b>The Analysis methodology</b>\n",
        "</h2>\n",
        "<p>\n",
        "For this I used the typical data science methodology <a href=\"https://aiden-dataminer.medium.com/the-data-science-method-dsm-a-framework-on-how-to-take-your-data-science-projects-to-the-next-91f9fd81e5d1\">which comprise of the following 6 steps</a>:\n",
        "<ol>\n",
        "<li> <a href=\"#ref_sec_1\">Problem identification</a></li>\n",
        "<li> <a href=\"#ref_sec_2\">Data Wrangling </a></li>\n",
        "<li><a href=\"#ref_sec_3\">Exploratory Data Analysis</a> </li>\n",
        "<li> <a href=\"#ref_sec_4\">Pre-processing and Training Data Development</a></li>\n",
        "<li><a href=\"#ref_sec_5\">Modeling </a> </li>\n",
        "<li> <a href=\"#ref_sec_6\">Conclusion </a></li>\n",
        "</ol>\n",
        "</p>\n",
        "</section>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<section>\n",
        "<h2>\n",
        "<b>Data Wrangling</b>\n",
        "</h2>\n",
        "<a name=\"ref_sec_2\">\n",
        "<p>\n",
        "The major challenge in the current project is to find the appropriate data for the whole data science pipeline. Since there was no standalone data for the current work. I decided to create my own. The first decision which I took towards creating my dataset is to look for stocks that are vulnerable to manipulations. To find such stocks can be tricky as the companies trading on the major stock exchanges are governed by tighter regulations or to put it in another way they are less risky. Therefore, I decided to look for more risky companies. I searched on the <a href=\"https://www.sec.gov/spotlight/microcap-fraud.shtml\">security and exchange commission and found that microcap stocks</a> trading on the  \"OTC\" markets are the most eligible candidates for this. I downloaded the <a href=\"https://www.otcmarkets.com/research/stock-screener\"> OTC market screener</a> and using the pandas data reader module started downloading the historical stock prices (these are by default not intraday price movements). The downloaded data has a number of attributes like \"Open\", \"Close\", \"High\", \"Low\", \"Adjusted Close\" and <a href=\"https://www.investopedia.com/terms/v/volume.asp\">\"Volume\"</a> for a particular stock ticker. I computed an average price for each day by simply averaging the four prices $$ Average~Price=\\frac{Open+Close+High+Low}{4} $$. I also kept the trade volume in the data set for the stock prices. The data frame for the average stock prices looks like as shown. \n",
        "</p>\n",
        "\n",
        "\n",
        "</a>\n",
        "\n",
        "<!--<table border=\"1\" class=\"dataframe\">\n",
        "<caption><b>URL feature of Lending club</b></caption>-->\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "<caption><b>Computed average stock prices</b></caption>\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th>Symbols</th>\n",
        "      <th>RGBP</th>\n",
        "      <th>HCMC</th>\n",
        "      <th>DRNK</th>\n",
        "      <th>FTEG</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Date</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>2016-01-04</th>\n",
        "      <td>0.156525</td>\n",
        "      <td>853982.906250</td>\n",
        "      <td>0.000250</td>\n",
        "      <td>0.0001</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2016-01-05</th>\n",
        "      <td>0.151250</td>\n",
        "      <td>621587.562500</td>\n",
        "      <td>0.000300</td>\n",
        "      <td>0.0001</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2016-01-06</th>\n",
        "      <td>0.146455</td>\n",
        "      <td>402141.953125</td>\n",
        "      <td>0.000275</td>\n",
        "      <td>0.0001</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2016-01-07</th>\n",
        "      <td>0.147500</td>\n",
        "      <td>375787.476562</td>\n",
        "      <td>0.000228</td>\n",
        "      <td>0.0001</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "I also downloaded the tweets related to the tickers starting from '2016-01-01 8:00:00'-- '2021-04-10 8:00:00'. I used <a href=\"https://github.com/twintproject/twint\">twint</a> for that as there is no limit to the amount of tweets which can be extracted. This generated a dataset worth 10 million tweets. A glimpse of the generated dataset is as shown:--\n",
        "\n",
        "<!--<b>Feature leakage</b>\n",
        "</h3><p> Feature leakage is a situation in which a model is built using data which is not available at the time the model will be used to make a prediction. Considering this and after checking the description of the data in the lending club data dictionary--I will drop these features as these features may not be available at the time of classification and or may be too indicative of the target value.\n",
        "For example the recoveries feature is the \"post charge off recovery\"---this is only available after a charge off has occured and shouldn't be included in the model development.</p>-->\n",
        "<!--<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1iLBCmZoWAxkVoN95mK_Aka97wDQ3-UBn \"alt=\"image\" width=\"550\" height=\"500\">\n",
        "<figcaption><font size=\"+1\"><b>Data types in the Lending club data: significant of them are floats</b></font></figcaption>\n",
        "</center>\n",
        "</figure>-->\n",
        "\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "<caption><b>Tweets data</b></caption>\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Date</th>\n",
        "      <th>Ticker</th>\n",
        "      <th>Tweets</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>XTRM Which house  you like?</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>XTRM</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>XTRM</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>XTRM Realestate Hard asset</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>HCMC hcmc sndl tlry bb suti xtrm ctrm qban ubqu seek vism bigg mine mdnp brll fteg eeenf garb everything is ready for takeoff if you hold one of these titles you are about to be rewarded!</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>Join our Alerts &amp;amp; Options  GAXY RLFTF IGEN GAXY CCTL BRTXQ IDEX XTRM RBII OPTI AVXL JADA RNWF NVAX BA SSFT PBI KODK KNDI SQ TSM SPY NFLX M DIS O NKLA AAPL MARK MRNA USO MRO GE AMD MSFT TSLA PYPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>The bad days are over   Stacks OLT KSM MITH CAKE XMR DAO ZIL SFOR BAND XLM tlry Monero WDLF XOM CDAI CAKE RVC QKC Unibright OGN DOGE BDGR WEEDSTOCKS LEO MIOTA SC StocksToBuy TUSD VRSC LTC XTRM XMR JNJ MaidSafeCoin</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>Give me your next runner  wdlf neca mgon tlss ilus mmex maxd opti eeenf hcmc enzc imtl usei fteg zhud tptw vnue gtll gaxy qban hbrm phil apyp onci aitx ssok catv wsgf shmn wtii ilus pvdg xtrm hitif vnue prpm</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>2021-04-10</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>XTRM looks like California really needs your tech wildfires</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>2021-04-09</td>\n",
        "      <td>XTRM</td>\n",
        "      <td>HCMC hcmc sndl tlry bb suti xtrm ctrm qban ubqu seek vism bigg mine mdnp brll fteg eeenf garb everything is ready for takeoff if you hold one of its titles you are about to be rewarded! the consolidation is over time for a confirmation and big breakout</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "<p> The tweets data was typically messy as people use emojis in it and type in upper and lower case. I don't consider the spelling mistakes as a text processing task however, that can be another issue too. The textual data was then standardized by removing the punctuations, emojis and bringing them to the upper case, and stored in a CSV file. However, for sentiment feature generation I used Vader later on--which can handle punctuations and emojis so I used the text data with emojis and punctuations as input to the <a href=\"https://github.com/cjhutto/vaderSentiment\">Vader</a> sentiment analyzer. Nevertheless, textual cleaning by removing emojis, punctuations, and bringing to the upper case might help me in generating some more advanced features by named entity recognition and generating visualizations for exploratory data analysis, etc.</p>\n",
        "\n",
        "<h3>\n",
        "<b>Price Anomaly detection</b>\n",
        "</h3>\n",
        "\n",
        "<p>Price anmoalies were detected in the following manner for each ticker. First the ticker was selected and the date time index were recorded where the tweet volume for that particular <a href=\"https://money.cnn.com/2012/07/31/technology/twitter-cashtag/index.htm\">cashtag</a> is more than 100. Once the date time index for such days were recorded then I computed the backward looking average (\\( \\mu \\)) and standard deviation (\\( \\sigma \\))  for the past 20 days. Once these two measures where established then I created indicators by using the following formula. $$ Indicator = \\Big\\{  1~Average~price >= 2\\sigma+\\mu $$, $$ Indicator = \\Big\\{  0~Average~price < 2\\sigma+\\mu $$. Since, the sheer volume of the tweets is relatively large so I only decided to start the calculation for 600 tickers to begin with. This generated a dataset more than 14,000 points with a significant amount of textual data, price and volume of the stocks. The final dataset looks like as shown.  \n",
        "</p> \n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "<caption><b>Dataset after the initial processing</b></caption>\n",
        "\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Price</th>\n",
        "      <th>Ticker</th>\n",
        "      <th>Volume</th>\n",
        "      <th>Sentiment_Score</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Date</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>2016-09-16</th>\n",
        "      <td>0.147512</td>\n",
        "      <td>RGBP</td>\n",
        "      <td>731072.0</td>\n",
        "      <td>0.357143</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2016-09-20</th>\n",
        "      <td>0.139328</td>\n",
        "      <td>RGBP</td>\n",
        "      <td>1570256.0</td>\n",
        "      <td>0.654397</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2016-09-21</th>\n",
        "      <td>0.125200</td>\n",
        "      <td>RGBP</td>\n",
        "      <td>491014.0</td>\n",
        "      <td>0.293478</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2016-09-22</th>\n",
        "      <td>0.137250</td>\n",
        "      <td>RGBP</td>\n",
        "      <td>1041242.0</td>\n",
        "      <td>0.189573</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2016-09-29</th>\n",
        "      <td>0.126885</td>\n",
        "      <td>RGBP</td>\n",
        "      <td>346951.0</td>\n",
        "      <td>0.236994</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>While building the above dataset I also computed the net sentiment for all the tweets via the vader sentiment analyzer.</p>\n",
        "</section>\n",
        "\n",
        "\n",
        "\n",
        "<section>\n",
        "<h2>\n",
        "<b>Exploratory Data Analysis</b>\n",
        "</h2>\n",
        "<a name=\"ref_sec_3\">\n",
        "<p>\n",
        "The density of this dataset provided me a unique opportunity to explore it in a variety of ways. By grouping the data along various features I was able to explore it in various different ways. I first tried to understand how the customers of lending are distributed among different \"grades\" by simply counting it:\n",
        "</p>\n",
        "\n",
        "\n",
        "</a>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1--S7kMvp6UL1sHYpRB8ARs5vZWoMEeSC \"alt=\"image\" width=\"750\" height=\"500\">\n",
        "<figcaption><font size=\"+1\"><b>How the customers are distributed among different grades?</b></font></figcaption>\n",
        "</center>\n",
        "<p>It appears that the target customers are in mainly \"B\" and \"C\" category and not in the \"A\"</p>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-0Bi9vbjTNSCabpT--xAQUNO4ysIBWmu\"alt=\"image_2\" width=\"550\" height=\"500\">\n",
        "<figcaption><font size=\"+1\"><b> The sankey chart shows the relationship between the grades, home ownership and loan amounts</b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>The thickness of the links between the customer grades and the home ownership status represents the count of the loans. It is obvious from here that the preferred customers are those which are currently under a mortgage plan followed by rent and own.</p>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-0qHmhLmY0QziZ7QT-5EtFLGNuhU_tYx\"alt=\"image_3\" width=\"750\" height=\"500\">\n",
        "<figcaption><font size=\"+1\"><b>More concretely a $49.2\\%$, $39.6\\%$ and $11.2\\%$ of the total customers are with mortgages, rent and own respectively. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<p><b>How is the fico score distributed?</b></p>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-256bNKeI85qA573KttC7eQlxQ9rW8Q_\"alt=\"image_4\" width=\"550\" height=\"500\">\n",
        "<figcaption><font size=\"+1\"><b> Most of the lenders are in the range of 670~800 range. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<p><b>What is the relation between fico range, grades and loan?</b></p>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-FTS03M8L1kc5mJOitDRRPlE_caSMcJx\"alt=\"image_4\" width=\"550\">\n",
        "<figcaption><font size=\"+1\"><b> The sankey chart shows the relationship between the grades, fico range and loan amount counts-- as again 660-670 fico range is the preferred. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<p><b>Is the loan coming back to lending club?</b></p>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-6ZSXhlU57tgjPLxmmmeHH00QFhnevgi\"alt=\"image_6\" width=\"550\" height=\"500\">\n",
        "<figcaption><font size=\"+1\"><b> Lots of fully paid loans! </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>Lending club has a high number of customers either in the Fully paid and or Current range. This means that the company's model of lending the money is working well for them--the money which is going out is coming back too with some interest paid</p>\n",
        "\n",
        "</section>\n",
        "\n",
        "<p> <b>What is the employment history of the lenders?</b></p>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-Bt5mJh6Kh2aSLhnEOZxza_SPb084TCj\"alt=\"image_7\" width=\"550\" height=\"500\">\n",
        "<figcaption><font size=\"+1\"><b> Most of the lenders have the employment history of more than 10 years. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "<p>By grouping the data with employment length shows that the lending club customers have signficant number of people who have a good employment history i.e. more than 10+ years.</p>\n",
        "\n",
        "<p><b>What is the relationship between \"mean\" loan amount, installment and interest rate within each category?</b></p>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1f5yXzKDQeWhMmstRn450Fm6Ga79QKOzn\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> Bubble plot of \"mean\" loan amount vs mean \"interest rate\" features. Here the size of the bubble is related to the \"mean\" interest rate while the color of the bubbles represents the categories (A,B,C,D,E,F and G). </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "<p>Here I have grouped the data by grades and plotted them as bubble chart. Mean loan amount and mean installments show a linear behavior while the meant interest rate tend to grow sharply as we move from category B to G with an exception of category A which falls in between the categroy B and C.</p>\n",
        "\n",
        "<p><b> At a more granular level how the interest rate is distributed within each category?</b></p>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1--vqbsXMJUCyCiJ2i13Kfe-vg8XHS6xV\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> Interest rate distribution within each category of lenders. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p> There is a very clear trend here among the interest within the categories $A < B < C < D$.</p>\n",
        "\n",
        "<p><b>For which purpose the loan was taken?</b></p>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1McN8Z7rtzWosBVpc4Rgoy3JU73ul6Wwz\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> The \"mean\" loan is the highest for the small business followed by debt consolidation and so on. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "<p> The \"mean value\" of the loan is highest for the small business purpose.</p>\n",
        "\n",
        "\n",
        "<p><b>How the median of the loan amount, interest rate and annual income is distributed accross the states?</b></p>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-7hsSNysKZfrMjID16XQNhO8gW5HTOZN\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> How the median loan is distributed among different states of the US? </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-Ea3XxvUr3QeV8eEYbH72M91jy8Ll83t\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> How the median annual income is distributed among different states of the US? </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p><b>How the charged off and the people who returned the loan are distributed accross the states?</b></p>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-HBiwX4AfdyTG83ej6VzVa42bhgS70wa\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> How the charged off is distributed among different states--California leads the pack followed by New York. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-L-9RJ0T2u4-rDnGVFoiufWEkhqpdp3b\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> How the current loan status is distributed among different states--California leads the pack followed by Texas and New York. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p><b>How the interest rate is distributed between different categories of loan status?</b></p>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-OF_F4CtH_h06fP4t2mZxkyaRZB0EPwD\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> How the interest rate is distributed among different categories of loan status. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>The interest rate is lower for the payers category and higher for the charged off category on an average. </p>\n",
        "<p><b>Uptill now I have plotted either the average values of the various features of the data in a variety of ways. Further to get a better estimate of the statistical features of the data I will make use of the violin plot for the interest rate on grade and states etc.</b></p>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-RrlPtOkH7iTCMnEorSPMZykKmXFuSEE\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> Violin plot of the interest among different grades of the customers.</b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>Not only the grade 'A' has the minimal value of the median. The dispersion of interest is the least in category A. Clearly from the sheer interest rate point of view it is good to be in this categroy. The above plot further differentiate the data by their term period.</p>\n",
        "\n",
        "<p><b>To see a rough value of the correlation between variables I will do a scatter matrix plot</b></p>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-S8gZOuPqAKDQJXEWKAQB45uKk6mQVyO\"alt=\"image_7\" width=\"750\" height=\"600\">\n",
        "<figcaption><font size=\"+1\"><b> Scatter plot of the various features of the data. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>Is there are a strong monotonic relationship between interest rate and loan amount? We can explore it using Spearmans Rank correlation coefficient.\n",
        "More can be learned about it from this great resource <a href=\"https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php\"> on Spearman's Rank-Order Correlation</a> Its definition is$\\frac{1-6\\sum d_{i}^2}{n(n^2-1)}$ here 'd' is the distance between the ranks of the data point.</p>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-UIByWvKiBOqF4rhEsR6IsQ5vOJXwzb2\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> Computed Spearman's correlation between loan amount, installment and interest rate.</b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>Very high spearman rank correlation means as we increase the rank of one the other will increase montonically. This is true for loan and installment. Means these two variables are highly correlated and increase monotonically. While the other parameters doesn't increase monotonically.</p>\n",
        "\n",
        "\n",
        "<section>\n",
        "<h2>\n",
        "<b>Training data development</b>\n",
        "</h2>\n",
        "<a name=\"ref_sec_4\">\n",
        "<p>\n",
        "<b>Removing the highly correlated features</b>\n",
        "<p>In this part I first removed the highly correlated variables. This is easier to do for the numerical data but not very straightforward for the categorical data. For the categorical data I used the cramers-V correlation and Thiel Uncertainty coefficient.</p>\n",
        "\n",
        "</a>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-_VynMd3STgWAm0BUxdF0M7I4-b-334w\"alt=\"image_7\" width=\"850\" height=\"750\">\n",
        "<figcaption><font size=\"+1\"><b> Heatmap of the correlation matrix. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>How much correlated categorical features are? To compute the correlation between categorical value I took help of this article in which the author provides a link to his kaggle kernel <a href=\"https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\"></a>. From the basic equations of the pearson's correlation coefficient it can be gleaned that it is not designed to handle the categorical features. Some other method need to be defined. One such approach is the Cramer's V correlation which is a symmetrical measure and varies from 0-1.</p>\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        " <thead>\n",
        " <tr style=\"text-align: right;\">\n",
        " <th></th>\n",
        " <th>variable_1</th>\n",
        " <th>variable_2</th>\n",
        " <th>Thiel U</th>\n",
        " </tr>\n",
        " </thead>\n",
        " <tbody>\n",
        " <tr>\n",
        " <th>0</th>\n",
        " <td>loan_status</td>\n",
        " <td>next_pymnt_d</td>\n",
        " <td>0.644694</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>1</th>\n",
        " <td>loan_status</td>\n",
        " <td>last_pymnt_d</td>\n",
        " <td>0.602468</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>2</th>\n",
        " <td>loan_status</td>\n",
        " <td>last_credit_pull_d</td>\n",
        " <td>0.278253</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>3</th>\n",
        " <td>loan_status</td>\n",
        " <td>issue_d</td>\n",
        " <td>0.267395</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>4</th>\n",
        " <td>loan_status</td>\n",
        " <td>title</td>\n",
        " <td>0.065845</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>5</th>\n",
        " <td>loan_status</td>\n",
        " <td>initial_list_status</td>\n",
        " <td>0.033257</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>6</th>\n",
        " <td>loan_status</td>\n",
        " <td>settlement_date</td>\n",
        " <td>0.031000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>7</th>\n",
        " <td>loan_status</td>\n",
        " <td>debt_settlement_flag_date</td>\n",
        " <td>0.030703</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>8</th>\n",
        " <td>loan_status</td>\n",
        " <td>settlement_status</td>\n",
        " <td>0.030590</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>9</th>\n",
        " <td>loan_status</td>\n",
        " <td>debt_settlement_flag</td>\n",
        " <td>0.030425</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>10</th>\n",
        " <td>loan_status</td>\n",
        " <td>sub_grade</td>\n",
        " <td>0.028570</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>11</th>\n",
        " <td>loan_status</td>\n",
        " <td>grade</td>\n",
        " <td>0.026454</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>12</th>\n",
        " <td>loan_status</td>\n",
        " <td>disbursement_method</td>\n",
        " <td>0.019841</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>13</th>\n",
        " <td>loan_status</td>\n",
        " <td>sec_app_earliest_cr_line</td>\n",
        " <td>0.018732</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>14</th>\n",
        " <td>loan_status</td>\n",
        " <td>earliest_cr_line</td>\n",
        " <td>0.018138</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>15</th>\n",
        " <td>loan_status</td>\n",
        " <td>application_type</td>\n",
        " <td>0.016498</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>16</th>\n",
        " <td>loan_status</td>\n",
        " <td>verification_status_joint</td>\n",
        " <td>0.015232</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>17</th>\n",
        " <td>loan_status</td>\n",
        " <td>verification_status</td>\n",
        " <td>0.007668</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>18</th>\n",
        " <td>loan_status</td>\n",
        " <td>payment_plan_start_date</td>\n",
        " <td>0.004565</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>19</th>\n",
        " <td>loan_status</td>\n",
        " <td>hardship_end_date</td>\n",
        " <td>0.004536</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>20</th>\n",
        " <td>loan_status</td>\n",
        " <td>hardship_start_date</td>\n",
        " <td>0.004467</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>21</th>\n",
        " <td>loan_status</td>\n",
        " <td>hardship_status</td>\n",
        " <td>0.004404</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>22</th>\n",
        " <td>loan_status</td>\n",
        " <td>hardship_reason</td>\n",
        " <td>0.003484</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>23</th>\n",
        " <td>loan_status</td>\n",
        " <td>hardship_loan_status</td>\n",
        " <td>0.003372</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>24</th>\n",
        " <td>loan_status</td>\n",
        " <td>hardship_type</td>\n",
        " <td>0.003192</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>25</th>\n",
        " <td>loan_status</td>\n",
        " <td>purpose</td>\n",
        " <td>0.003024</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>26</th>\n",
        " <td>loan_status</td>\n",
        " <td>emp_length</td>\n",
        " <td>0.002125</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>27</th>\n",
        " <td>loan_status</td>\n",
        " <td>home_ownership</td>\n",
        " <td>0.001983</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>28</th>\n",
        " <td>loan_status</td>\n",
        " <td>hardship_flag</td>\n",
        " <td>0.001689</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>29</th>\n",
        " <td>loan_status</td>\n",
        " <td>pymnt_plan</td>\n",
        " <td>0.001344</td>\n",
        " </tr>\n",
        " </tbody>\n",
        " <caption><b>Thiel U coefficient table</b></caption>\n",
        "</table>\n",
        "\n",
        "<p>From Cramers V correlation it can be said that some of the features are highly correlated with each other (grade and sub_grade) while the Theil U coefficient inform about the importance of a particular feature which indicates directly towards the target variable. So first I will keep those object type variables which have good enough information about the target variables and then remove the object type variables which are highly correlated among themselves.</p>\n",
        "<p><b>Encoding the categorical features</b></p>\n",
        "<p>Some of the object features are of very high cardinality. one hot encoding will make a feature explosion.<a href=\"http://contrib.scikit-learn.org/category_encoders/index.html\">The following link and library mentions about some ways to deal with features with very high cardinality</a>. I can use hashing but the hashing algorithm may put different categories in the same group--might affect the target negatively.<a href=\"https://maxhalford.github.io/blog/target-encoding/\"> Other way around is the target encoding which is explained quite well in the link which can be accessed by clicking here.</a>\n",
        "\n",
        "I could have used the mean encoding. However, I decide to follow the Laplace smoothing of the local mean. It is because if there is a very low occurence of a certain instance of a feature then its local mean cannot be trusted. Then a more heavy emphasis should be given on the global mean.</p>\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "<caption></caption>\n",
        " <thead>\n",
        " <tr style=\"text-align: right;\">\n",
        " <th></th>\n",
        " <th>less_than_200_counts_categories</th>\n",
        " <th>greater_than_200_counts_categories</th>\n",
        " <th>unique_category</th>\n",
        " <th>ratio</th>\n",
        " </tr>\n",
        " </thead>\n",
        " <tbody>\n",
        " <tr>\n",
        " <th>grade</th>\n",
        " <td>1.0</td>\n",
        " <td>7.0</td>\n",
        " <td>8</td>\n",
        " <td>7.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>emp_length</th>\n",
        " <td>0.0</td>\n",
        " <td>12.0</td>\n",
        " <td>12</td>\n",
        " <td>NaN</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>verification_status</th>\n",
        " <td>1.0</td>\n",
        " <td>3.0</td>\n",
        " <td>4</td>\n",
        " <td>3.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>issue_d</th>\n",
        " <td>14.0</td>\n",
        " <td>126.0</td>\n",
        " <td>140</td>\n",
        " <td>9.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>purpose</th>\n",
        " <td>1.0</td>\n",
        " <td>14.0</td>\n",
        " <td>15</td>\n",
        " <td>14.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>earliest_cr_line</th>\n",
        " <td>235.0</td>\n",
        " <td>520.0</td>\n",
        " <td>755</td>\n",
        " <td>2.212766</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>initial_list_status</th>\n",
        " <td>1.0</td>\n",
        " <td>2.0</td>\n",
        " <td>3</td>\n",
        " <td>2.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>last_pymnt_d</th>\n",
        " <td>32.0</td>\n",
        " <td>105.0</td>\n",
        " <td>137</td>\n",
        " <td>3.281250</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>next_pymnt_d</th>\n",
        " <td>104.0</td>\n",
        " <td>3.0</td>\n",
        " <td>107</td>\n",
        " <td>0.028846</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>last_credit_pull_d</th>\n",
        " <td>56.0</td>\n",
        " <td>86.0</td>\n",
        " <td>142</td>\n",
        " <td>1.535714</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>application_type</th>\n",
        " <td>1.0</td>\n",
        " <td>2.0</td>\n",
        " <td>3</td>\n",
        " <td>2.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>verification_status_joint</th>\n",
        " <td>0.0</td>\n",
        " <td>4.0</td>\n",
        " <td>4</td>\n",
        " <td>NaN</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>sec_app_earliest_cr_line</th>\n",
        " <td>462.0</td>\n",
        " <td>202.0</td>\n",
        " <td>664</td>\n",
        " <td>0.437229</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>hardship_status</th>\n",
        " <td>0.0</td>\n",
        " <td>4.0</td>\n",
        " <td>4</td>\n",
        " <td>NaN</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>disbursement_method</th>\n",
        " <td>1.0</td>\n",
        " <td>2.0</td>\n",
        " <td>3</td>\n",
        " <td>2.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>debt_settlement_flag</th>\n",
        " <td>1.0</td>\n",
        " <td>2.0</td>\n",
        " <td>3</td>\n",
        " <td>2.000000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>debt_settlement_flag_date</th>\n",
        " <td>56.0</td>\n",
        " <td>28.0</td>\n",
        " <td>84</td>\n",
        " <td>0.500000</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>settlement_status</th>\n",
        " <td>0.0</td>\n",
        " <td>4.0</td>\n",
        " <td>4</td>\n",
        " <td>NaN</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>settlement_date</th>\n",
        " <td>61.0</td>\n",
        " <td>30.0</td>\n",
        " <td>91</td>\n",
        " <td>0.491803</td>\n",
        " </tr>\n",
        " </tbody>\n",
        "</table>\n",
        "\n",
        "<p>It means there are 14 columns with less than 200 counts and all the other columns have higher count. I decided to work with the local mean where the count is higher than 200 else I decided to divert the mean towards the global mean.</p>\n",
        "<p><b>Feature leakage</b>:-- leakage is a situation in which a model is built using data which is not available at the time the model will be used to make a prediction. Considering this and after checking the description of the data in the lending club data dictionary--I will drop these features <q>issue_d_encoded,total_rec_late_fee,debt_settlement_flag_encoded,settlement_status_encoded,settlement_amount,settlement_date_encoded,debt_settlement_flag_date_encoded,settlement_amount,last_pymnt_d_encoded,settlement_date_encoded,next_pymnt_d_encoded,recoveries,out_prncp,last_pymnt_amnt,last_credit_pull_d_encoded,last_fico_range_high,total_pymnt</q> as these features may not be available at the time of classification and or may be too indicative of the target value. For example the recoveries feature is the \"post charge off recovery\"---this is only available after a charge off has occured and shouldn't be included in the model development.</p>\n",
        "</section>\n",
        "\n",
        "<section>\n",
        "\n",
        "<h2>\n",
        "<b>Modeling</b>\n",
        "</h2>\n",
        "<a name=\"ref_sec_5\">\n",
        "<p>\n",
        "<b>Outlier removal</b> </p>\n",
        "<p>\n",
        "Outliers can create some problems for the machine learning algorithms further down the line and lead to incorrect conclusions. If there is any specially in the float data then I will remove it using the quantile method.\n",
        "</p>\n",
        "</a>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1KJ2tcxH-R76iTeCtBJyleChRDeeWVqZ0\"alt=\"image_7\" width=\"950\" height=\"550\">\n",
        "<figcaption><font size=\"+1\"><b> Raw loan vs the log transformed data. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        " <thead>\n",
        " <tr style=\"text-align: right;\">\n",
        " <th></th>\n",
        " <th>index</th>\n",
        " <th>annual_inc</th>\n",
        " <th>loan_amnt</th>\n",
        " <th>purpose</th>\n",
        " </tr>\n",
        " </thead>\n",
        " <tbody>\n",
        " <tr>\n",
        " <th>0</th>\n",
        " <td>24659</td>\n",
        " <td>8700000.0</td>\n",
        " <td>14000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>1</th>\n",
        " <td>29400</td>\n",
        " <td>6000000.0</td>\n",
        " <td>18500.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>2</th>\n",
        " <td>38473</td>\n",
        " <td>7000000.0</td>\n",
        " <td>25000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>3</th>\n",
        " <td>40533</td>\n",
        " <td>9000000.0</td>\n",
        " <td>11000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>4</th>\n",
        " <td>48075</td>\n",
        " <td>8500021.0</td>\n",
        " <td>12000.0</td>\n",
        " <td>credit_card</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>5</th>\n",
        " <td>75685</td>\n",
        " <td>5000010.0</td>\n",
        " <td>20000.0</td>\n",
        " <td>credit_card</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>6</th>\n",
        " <td>85757</td>\n",
        " <td>8253000.0</td>\n",
        " <td>30000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>7</th>\n",
        " <td>128385</td>\n",
        " <td>8121180.0</td>\n",
        " <td>5000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>8</th>\n",
        " <td>132773</td>\n",
        " <td>7600000.0</td>\n",
        " <td>10000.0</td>\n",
        " <td>home_improvement</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>9</th>\n",
        " <td>205670</td>\n",
        " <td>6000000.0</td>\n",
        " <td>35000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>10</th>\n",
        " <td>209413</td>\n",
        " <td>6000000.0</td>\n",
        " <td>4475.0</td>\n",
        " <td>home_improvement</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>11</th>\n",
        " <td>217721</td>\n",
        " <td>8900060.0</td>\n",
        " <td>10550.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>12</th>\n",
        " <td>230972</td>\n",
        " <td>9500000.0</td>\n",
        " <td>24000.0</td>\n",
        " <td>credit_card</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>13</th>\n",
        " <td>315219</td>\n",
        " <td>7000000.0</td>\n",
        " <td>7500.0</td>\n",
        " <td>car</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>14</th>\n",
        " <td>400638</td>\n",
        " <td>8706582.0</td>\n",
        " <td>8000.0</td>\n",
        " <td>credit_card</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>15</th>\n",
        " <td>444152</td>\n",
        " <td>8020871.0</td>\n",
        " <td>35000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>16</th>\n",
        " <td>461167</td>\n",
        " <td>8365188.0</td>\n",
        " <td>10000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>17</th>\n",
        " <td>473702</td>\n",
        " <td>6032121.0</td>\n",
        " <td>25000.0</td>\n",
        " <td>debt_consolidation</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>18</th>\n",
        " <td>474214</td>\n",
        " <td>7500000.0</td>\n",
        " <td>35000.0</td>\n",
        " <td>credit_card</td>\n",
        " </tr>\n",
        " <tr>\n",
        " <th>19</th>\n",
        " <td>539807</td>\n",
        " <td>10999200.0</td>\n",
        " <td>5000.0</td>\n",
        " <td>major_purchase</td>\n",
        " </tr>\n",
        " </tbody>\n",
        " <caption><b>Dataframe where the income and purpose of the loan are put togethere side by side. Only those lenders are selected who earn in $7$ figures. Quite a few people are asking for \"debt consolidation\".</b></caption>\n",
        "</table>\n",
        "<p>Log transforming present a clear case that there are too many entries above and below the mean--specially there are quite a few values close to million. This can't be a human error. Further grouping the data by loan and purpose I can see that a person earning a $9$ figure salary is taking loan for debt consolidation(?). This looks a little off but it cannot stop this person for taking loan as anyone can take loan for any purpose. And there are quite a few entries in the dataset where the lender earns in millions and is asking for a loan for debt consolidation.</p>\n",
        "\n",
        "<p><b>Model training</b>\n",
        "Since, this is a classification problem and the way to categorize the correctness of the trained model is rather subjective and is based on the idea of confusion matrix.\n",
        "However, the following measures are readily available for the classification problem:</p>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1n97eKj1HvOdE-cg2OTc1YlvT0X8VmjcV\"alt=\"image_7\" width=\"550\" height=\"400\">\n",
        "<figcaption><font size=\"+1\"><b> Confusion matrix. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<ul>\n",
        "<li>For example Accuracy = $\\frac{\\text{Total number of correct predictions}}{\\text{Total predictions made}}$</li>\n",
        "<li> Precision = $\\frac{\\text{True positive}}{\\text{True positive + False positive}}$</li>\n",
        "<li>Recall = $\\frac{\\text{True positive}}{\\text{True positive + False negative}}$</li>\n",
        "<li>F1 score= $\\frac{\\text{2* Precision*Recall}}{\\text{Precision+Recall}}$</li>\n",
        "<li> MCC = $\\frac{TP*TN-FP*FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN}}$. </li>\n",
        "</ul>\n",
        "<p>Considering this I will optimize for Recall as I don't want someone to be categorized as non defaulter even though he/she is going to do a default.</p>\n",
        "<ul>\n",
        "<li> <b>Logistic regression</b> </li> For solving the classification problem I will start with the simplest classifier aka the logistic regression.\n",
        "</ul>\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-AMMp37Oe98DkkX7mEdilCjsJAG0H6Z_\"alt=\"image_7\" width=\"450\"/> <img src=\"https://drive.google.com/uc?id=1AkCK42jQfrliaknd-rArzERiErbJXOR0\"alt=\"image_7\" width=\"450\"/>\n",
        "<figcaption><font size=\"+1\"><b> Receiver operating characteristic (ROC) and precision recall curve for a tuned (hyperparameter tunning) logistic regression model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1-Oh3BRzDM4i169-FdI6UNqEoG3zM-EVk\"alt=\"image_7\" width=\"550\" >\n",
        "<figcaption><font size=\"+1\"><b> Confusion matrix for the tuned logistic regression model. </b></font></figcaption>\n",
        "</figure>\n",
        "</center>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-1RcIMmTwBcXgdIThu9civShGlHS3QDK\"alt=\"image_7\" width=\"550\" >\n",
        "<figcaption><font size=\"+1\"><b> Which feature got the highest significance in the tuned logistic regression model </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "<p>Logistic regression puts the highest importance on the annual income followed by number of derogatory public records--all of which makes complete sense. However, I am not satisfied by its higher recall value considering this I will try some ensemble learning techniques.</p>\n",
        "<ul>\n",
        "<li> <b>Light gradient boosting</b> </li>\n",
        "</ul>\n",
        "\n",
        "<figure>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-EOkVRfaH_kyBZHFACutCGfDHosE6jei\"alt=\"image_7\" width=\"450\"/> <img src=\"https://drive.google.com/uc?id=1Ij9u6yYoBu5F5gy8q_m_n16Fwh6Da5b8\"alt=\"image_7\" width=\"450\"/>\n",
        "<figcaption><font size=\"+1\"><b> Receiver operating characteristic (ROC) and precision recall curve for a tuned (hyperparameter tunning) light gradient boosting model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-SElO4DWxPAtFS2chO7sunNkS83aIFVx\"alt=\"image_7\" width=\"550\" >\n",
        "<figcaption><font size=\"+1\"><b> Confusion matrix for the tuned light gradient boosting model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-MyaYOb__L28JdADNGgxRP_zXKJ-d8Fn\"alt=\"image_7\" width=\"550\" >\n",
        "<figcaption><font size=\"+1\"><b> Which feature got the highest significance in the tuned light gradient boosting model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>Surprisingly this model performs poorly than the logistic regression. It has a higher false positive rate while the true positive rate is also low. Meaning this must be having a lower value of recall.</p>\n",
        "\n",
        "\n",
        "<ul>\n",
        "<li> <b>Xtreme gradient boosting</b> </li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-UAoBAWSX2PZH7V293nkDobtLlin0kDx\"alt=\"image_7\" width=\"450\"/> <img src=\"https://drive.google.com/uc?id=1--QYRt3mG_UiQG3kkQVQZBO-JyLJZVnD\"alt=\"image_7\" width=\"450\"/>\n",
        "<figcaption><font size=\"+1\"><b> Receiver operating characteristic (ROC) and precision recall curve for a tuned (hyperparameter tunning) XGBOOST model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1-WK2h_qw0lqNgOtAeohoY-lUw-GXju72\"alt=\"image_7\" width=\"550\">\n",
        "<figcaption><font size=\"+1\"><b> Confusion matrix for XGBOOST model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1dG02hpM7jfBzwevodA2gbGo351oeRcFK\"alt=\"image_7\" width=\"550\">\n",
        "<figcaption><font size=\"+1\"><b> Which feature got the highest importance in XGBOOST. </b></font></figcaption>\n",
        "</center>\n",
        "\n",
        "</figure>\n",
        "<p>XGBOOST has a very low value of false negative and a higher true positive rate. This means its recall should be better.</p>\n",
        "\n",
        "\n",
        "\n",
        "<ul>\n",
        "<li> <b>Catboost model</b> </li>\n",
        "</ul>\n",
        "<p>At the end I tried categorical boosting without the target encoding performed on the dataset. This is because the catboost model is specifically designed to work with categorical data.</p>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1Q2uFgZv003bawjnUgPeP6OGLGEHncZZv\"alt=\"image_7\" width=\"450\"/> <img src=\"https://drive.google.com/uc?id=1-j8v2h5WkSpJDVdL4mk2BAtdBcsZo69z\"alt=\"image_7\" width=\"450\"/>\n",
        "<figcaption><font size=\"+1\"><b> Receiver operating characteristic (ROC) and precision recall curve for a tuned (hyperparameter tunning) Catboost model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<p>The recall for tuned catboost model tend to be very low. In fact it performed poorer than the Xgboost model. This need some fine tuning and reconsideration of the feature cleanup.</p>\n",
        "\n",
        "<ul><li><b>Explaining the affect of the features on the model outcome</b></li></ul>\n",
        "\n",
        "<p>How much has each feature value contributed to the prediction compared to the average prediction?\n",
        "The answer to this question lies in the Shap values--which essentially tells how much a change in feature will drive the output away from the average prediction. <a href=\"https://christophm.github.io/interpretable-ml-book/shap.html\">A typical example of the house price prediction is given here</a> which explains the idea quite well. For linear model the driver which will move the output away from average will be the weight times the feature value but for non-linear model this is not straightforward to answer. The Shap values are model agnostic and exactly explains the same. SHAP values has their origin in the game theory. The “game” here is the prediction task for a single instance of the dataset. The “players” are the feature values of the instance that collaborate to play the game (predict a value)--i.e. the loan is going to get charged off or not. In short Shapley values correspond to the contribution of each feature towards pushing the prediction away from the expected value of the model outcome. The above analysis was for shap values for each feature of every observation, it is possible to get a global interpretation using Shapley values by looking at it in a combined form.\n",
        "</p>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1PTy9D4FsxbQWEBqpDB0DB9xCXWVGq2vt\"alt=\"image_7\" width=\"450\">\n",
        "<figcaption><font size=\"+1\"><b> SHAP values for the XGBOOST model. </b></font></figcaption>\n",
        "</center>\n",
        "</figure>\n",
        "<p>The SHAP value plot can show the positive and negative relationships of the predictors with the target variable. They do so by showing the following attributes:</p>\n",
        "<ol>\n",
        "<li>Feature importance: Variables are ranked in descending order.</li>\n",
        "<li>Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.</li>\n",
        "<li>Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.</li>\n",
        "<li>Correlation: A high value of the “grade_encoded” has a high and positive impact on the classification of a potential defaulter. The “high” comes from the red color, and the “positive” impact is shown on the X-axis.</li>\n",
        "\n",
        "</ol>\n",
        "\n",
        "<p>The shap values makes sense as I saw in the Sankey chart that there is a good correlation between the grades, house ownership and the loans. This means that the lending club tend to trust people which falls in the better category--which intuitively makes sense.</p>\n",
        "</section>\n",
        "\n",
        "<section>\n",
        "<h2>\n",
        "<b>Conclusion</b>\n",
        "</h2>\n",
        "<a name=\"ref_sec_6\">\n",
        "<p>\n",
        "A significant number of features in the current dataset are not indicative of the defaulters. A good feature engineering can lead to a simpler model--for example logistic regression. By performing the target encoding I was able to achieve decent result both with logistic regression and Xgboost models. Catboost is supposed to perform better however its abysmal recall value is not acceptable. In future I will try to improve the performance of catboost model by a more careful consideration. In the end the model results were explained using a metric derived from game theory i.e. the shap value.\n",
        "</p>\n",
        "</a>\n",
        "</section>"
      ]
    }
  ]
}